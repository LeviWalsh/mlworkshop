{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Workshop\n",
    "This tutorial we guide you through all aspects of ML covered in the presentation (and maybe more!)\n",
    "\n",
    "#### Notice:\n",
    "The goal of this notebook is to teach you the basics of making ML models in Tensorflow 2 and the Keras high level API. As such, we won't touch on data cleaning and creating effective datasets, which is a HUGE component of machine learning in the real world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load various libraries and setup\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.datasets import fashion_mnist, imdb, mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: The Fashion MNIST Dataset\n",
    "This data set is full of 28x28 images of various clothing items. We're using it for its consistency and essentially, lack of features, which makes it faster to train than most other datasets! it's also a lot more exciting than the MNIST digits dataset (and harder to classify).\n",
    "\n",
    "First, we're going to load the dataset and see what it looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "labels = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \\\n",
    "          \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
    "\n",
    "print(\"Training shape\", x_train.shape)\n",
    "print(\"labels shape\", y_train.shape)\n",
    "print(\"X data looks like this!\")\n",
    "fig, ax = plt.subplots(nrows=1, ncols=8, figsize=(12,6))\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    axi.imshow(x_train[i], cmap='gray')\n",
    "plt.show()\n",
    "print(\"y data looks like this\")\n",
    "print(y_train[:8])\n",
    "plt.clf()\n",
    "\n",
    "print(\"The data class are as follows:\")\n",
    "for i in range(len(labels)):\n",
    "    print(\"%d = %s\" % (i, labels[i]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Fashion MNIST\n",
    "We need to do a few operations to the dataset.\n",
    "\n",
    "#### 1. Floatify - the data is currenlty in ints -- let's make it floats\n",
    "#### 2. One-hot vector encode the labels\n",
    "So each one of our labels is unique and of the same size, we do what's called one hot encoding. This turns the label i into a vector of length c (where c is the number of classes) that is all zeros except a one at position i.\n",
    "\n",
    "#### 3. Flattening Data\n",
    "For DENSE NETWORKS ONLY, we're going to want flattened data! (aka each image is in a single column vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "num_classes = len(labels)\n",
    "\n",
    "# one-hot encode the training and testing labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# This is dense network! We want it to take in a column vector that is just the image. \n",
    "# So, we reshape the image by stacking!\n",
    "im_width, im_height = x_train[0].shape\n",
    "x_train_flat = x_train.reshape(len(x_train), im_width*im_height)\n",
    "x_test_flat = x_test.reshape(len(x_test), im_width*im_height)\n",
    "print(\"Flattened shape: \", x_train_flat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a basic model with Keras\n",
    "See this link for more detailed info: https://keras.io/getting-started/functional-api-guide/\n",
    "#### 1. Define the input layer.\n",
    "Usually this is keras.layers.Input !\n",
    "#### 2. Add to the graph\n",
    "This is where the model works! Layers are callable objects. We construct layers as follows:\n",
    "```\n",
    "input = Input(shape=, etc.)\n",
    "first_layer = Dense(shape=, etc.)(input)\n",
    "```\n",
    "By feeding the previous layer to the next layer, we connect their nodes in the computation graph!\n",
    "See the layers API: https://keras.io/layers/core/\n",
    "If you want, you can also mess around with layer initializers (how to initialize the weights etc.)\n",
    "Also look at activation functions: https://keras.io/activations/\n",
    "#### 3. Get the output.\n",
    "This is the end of your model, it's often referred to as the \"logits\"\n",
    "#### 4. Create a callable model object. \n",
    "`model = models.Model(input=input, output=output)`\n",
    "#### 5. Select a loss function\n",
    "https://keras.io/losses/\n",
    "Select a loss function to give the model. This is added at the end of the graph!\n",
    "#### 6. Select an optimizer!\n",
    "https://keras.io/optimizers/\n",
    "These are usually just variants of gradient descent. Note that the keras model API also lets you do all of this within `model.compile`\n",
    "#### Call fit to train!\n",
    "\n",
    "\n",
    "### A note on hyper-parameters\n",
    "Hyper-parameters are values related to the optimizer, loss, and other parts of the model that we can change to improve accuracy or convergence. We use whats called a _validdation_ dataset to asses how good our hyperparameters are. This is usually just a random subset of the training dataset, often around 20 percent.\n",
    "\n",
    "### Example\n",
    "Below is an example model for the fashion MNIST dataset. It's quite bad. This is to show you how to use the keras API.\n",
    "\n",
    "Note that this current model is literally just doing multiclass logistic regression! Its just a single dense layer connected to 10 output nodes with the softmax multiclass generalization of the logistic function.\n",
    "\n",
    "Your task is to improve the model later on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "# input layer to take in image\n",
    "inputs = layers.Input(shape=(784,))\n",
    "# A single dense layer (this is the output)\n",
    "output = layers.Dense(num_classes, activation='softmax')(inputs)\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output)\n",
    "optimizer = optimizers.SGD(lr=lr, decay=1e-6)\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train_flat, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we test the model!\n",
    "Notice that you'll probably get an accuracy that's a bit lower than your training or validation accuracy. This is because of _overfitting_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_flat, y_test)\n",
    "print(\"Model Overall scores: \")\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])\n",
    "\n",
    "# Example inference on a single image!\n",
    "# Note: we have to run expand_dims as keras expects batched input!\n",
    "pred = model.predict(np.expand_dims(x_test_flat[0], axis=0))[0]\n",
    "# Preds will return the logits or the values at the output layer! \n",
    "# The index with the max probability is our selected class\n",
    "print(\"Prediction probabilities: \", pred)\n",
    "pred = np.argmax(pred)\n",
    "print(\"Predicted class\", labels[pred], \"was class\", labels[np.argmax(y_test[0])])\n",
    "plt.imshow(x_test[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: A Dense Model\n",
    "Try to expand on the model I made to achieve higher accuracy using Dense Layers. I recommend looking into the following:\n",
    "1. Adding more Layers\n",
    "2. The ReLu activation function\n",
    "3. The dropout layer\n",
    "4. The RMS prop or adagrad optimizers\n",
    "5. Adjusting hyperparameters (learning rate, decay rate, etc.)\n",
    "6. Training more?\n",
    "\n",
    "### For convenience, I've repasted the starter code!\n",
    "I also seperated the training part from the model definition as it's very common to get shape errors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "# input layer to take in image\n",
    "inputs = layers.Input(shape=(784,))\n",
    "output = layers.Dense(num_classes, activation='softmax')(inputs)\n",
    "\n",
    "# TODO: ADD MORE STUFF HERE!!!\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output)\n",
    "optimizer = optimizers.SGD(lr=lr, decay=1e-6)\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model.fit(x_train_flat, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.evaluate(x_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Convolving!\n",
    "Let's try to improve the model using convolutional layers.\n",
    "Here are some steps to help guide you:\n",
    "1. What is our input shape now? Is it flattened?\n",
    "2. The Conv2D layer. Very useful. Be careful with shapes.\n",
    "3. How do we reduce the dimensionality of our data?\n",
    "4. Is our final layer any different?\n",
    "\n",
    "With about 2 convs I can get above 90 percent accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "# input layer to take in image\n",
    "inputs = layers.Input(shape=(None)) # TODO: add the correct shape! It's a gray scale image... \n",
    "\n",
    "# Hmmm, maybe a conv2d some where here? Here's an example:\n",
    "\n",
    "# you swim at a POOL\n",
    "\n",
    "output = None # TODO: What's the model output? Has it changed? If so how?\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output)\n",
    "optimizer = None # TODO Choose your optimizer\n",
    "model.compile(loss=losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train: What are we training on?\n",
    "model.fit(None, None, batch_size=batch_size, epochs=epochs, validation_split=0.2) # TODO: Fill in!\n",
    "### Test\n",
    "scores = model.evaluate(x_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: The IMDB Movies Dataset!\n",
    "Here we're going to explore NLP a bit more!\n",
    "\n",
    "The basic goal of the dataset is to classify movie reviews as either positive or negative. Again, let's load the data and see what it looks like!\n",
    "\n",
    "Note that we have to specify extra parameters. This is because keras does a lot of grunt data loading work for us. It automatically iterates through all of the words and indexes them by their frequency. For example, \"3\" encodes the 3rd most frequent word in the data. \n",
    "\n",
    "Later, you can go back and change how we load the data to get _better_ results by selecting better features!\n",
    "See https://keras.io/datasets/ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dataset parameters: ###\n",
    "# Feel free to change later!\n",
    "max_features = 5000 # The maximum number of unique words to include\n",
    "\n",
    "labels =[\"Negative\", \"Positive\"]\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=3)\n",
    "word_to_id = imdb.get_word_index()\n",
    "word_to_id = {k:(v + 3) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "id_to_word = { value:key for key, value in word_to_id.items()}\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print(\"Actual Example Data:\")\n",
    "print(x_train[0])\n",
    "print(\"Decoded Example Data: \")\n",
    "print(' '.join(id_to_word[id] for id in x_train[0] ))\n",
    "print(\"Class =\", y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: CNN For Text!\n",
    "Our data is already sequences of integers, but they aren't quite uniform enough for us to train on yet. They all need to be the same length! We do this by padding the end of the sequences with zeros after the review ends.\n",
    "\n",
    "We also need what's called an `Embedding` layer. Embedding layers turn positive integers (indexes) into dense vectors of fixed size. Essentially, they translate each word index into a vector that represents that word in a higher dimensional space (see presentation for reference). There are pretrained embeddings out there (GloVe) that will perform better because scientists spent years optimizing them to maximally represent the english langauge. However, they're big and a bit over kill for this, so we just have our model learn the representation instead.\n",
    "\n",
    "This time, I have defined the beginning of the model and the embedding layer for you, but feel free to change it's parameters!\n",
    "\n",
    "Your task is to complete the model below. As always, here is some guidance:\n",
    "1. Define input layers. Think: what size are the sequences?\n",
    "2. Were going to use a different type of convolution, as our filter only moves in 1D instead of 2. Hmmmmm..... Conv1D perhaps??\n",
    "3. We only have two classes this time, so we don't need a softmax. What was that function we used for logistic regression? oh wait. Note that it also has another name. Keras uses the other name.\n",
    "4. (Advanced Extra) Global max pooling works pretty well for this network! Dropout might also help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "maxlen = 300 # note that you can change this!\n",
    "x_train_seq = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_seq = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "embedding_dim = 50\n",
    "batch_size = 64\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen, embedding_dim))\n",
    "# The last param here specifies the dimension of the encoding space\n",
    "embeddings = layers.Embedding(max_features, 128)(inputs)\n",
    "output = layers.Conv1D(0, 0 , padding='valid', activation='', strides=0)(outputs) # TODO: Fix this conv layer\n",
    "\n",
    "# Maybe more convolutions? \n",
    "# (Note that unlike in images, convs mean something different for text. we might not need as many)\n",
    "# Maybe some pooling?\n",
    "# Maybe some dense layers to finish it off?\n",
    "\n",
    "# We only need 1 node for 2 classes :O\n",
    "output = layers.Dense(1, activation='')(output) # TODO: fill in the activation.\n",
    "\n",
    "model = models.Model(inputs=inputs, outputs=output)\n",
    "optimizer = None # TODO Choose your optimizer. I recommend adam.\n",
    "model.compile(loss=losses.binary_crossentropy, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train\n",
    "model.fit(x_train_seq, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "### Test\n",
    "scores = model.evaluate(x_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: ~ Time ~ for LSTMs\n",
    "Instead of spatial locality, let's now look at temporal locality!\n",
    "\n",
    "We're going to use LSTMs here, which are a bit more complex. \n",
    "\n",
    "Because LSTMs need to be trained in sequence, we often use a lower sequence length to increase training speed., while at the same time using more distinct features because we can encode more in our \"memory\" state. \n",
    "\n",
    "Your task is to complete the LSTM model.\n",
    "Here are some pointers:\n",
    "1. Look at the LSTM info. You _can_ use more than one LSTM layer. But you need to think. What does my LSTM return? Usually just the last state (hmm but LSTMs are trained on sequences...). What would more LSTMs correspond to? Training a sequence classifer on a sequence. The documentation or ask me about how to do this if you're interested after you get the basic model done. You should be able to get fairly good accuracy with just a single LSTM.\n",
    "2. Read the documetnation: https://keras.io/layers/recurrent/\n",
    "3. Dropout is _always_ a good idea for LSTMs. In fact, in Keras' LSTMs, dropout is built in on both ends. Wohoo!\n",
    "4. If you want, you can copy paste over everything from Task 3 after defining the model.\n",
    "5. Note that LSTMs take a long time to train. This is no exception, so try to make the model as small and simple as possible so you don't kill your computer or waste a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "max_features = 20000\n",
    "maxlen = 80 # note that you can change this!\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features, index_from=3)\n",
    "x_train_seq = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test_seq = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Parameters ###\n",
    "embedding_dim = 128 # I'm using more features now, so this is greater!\n",
    "batch_size = 32\n",
    "epochs = 3\n",
    "lr = 0.01\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen, embedding_dim))\n",
    "# The last param here specifies the dimension of the encoding space\n",
    "embeddings = layers.Embedding(max_features, 128)(inputs)\n",
    "# TODO: Complete LSTM here:\n",
    "output = layers.LSTM(None)\n",
    "\n",
    "# End the model with something? Maybe a dense hmmm?\n",
    "# If the LSTM does a lot of work, we shouldn't need much here.\n",
    "# Maybe borrow some code from the last section?\n",
    "\n",
    "# Compile the model!\n",
    "\n",
    "# Then fit the model to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train:\n",
    "model.fit(x_train_seq, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "### Test\n",
    "scores = model.evaluate(x_test_seq, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Generative Networks (challenge)\n",
    "\n",
    "This part is mainly to demonstrate some cool (but also small scale) stuff you can do with ML.\n",
    "\n",
    "This section walks you through code for a General Adversarial Network that generates digits from the MNIST dataset.\n",
    "\n",
    "GANs are made of two components: a generator and a discriminator.\n",
    "\n",
    "The discriminator is trained to determine wether or not an image is real or fake.\n",
    "The generator is trying to trick the discriminator, by turning noise into better data.\n",
    "\n",
    "The network is adversarial as it pits the discriminator and the generator against each other, with the generator trying to fool the discriminator and the discriminator trying to be as accurate as possible. The result: a generator network that can create real - looking digits from noise!\n",
    "\n",
    "![alt text](gan.png \"GAN\")\n",
    "\n",
    "The code for this part is a bit more complicated and requires a bit more knowledge of ML and Keras. Feel free to peruse and train!\n",
    "\n",
    "Currently, it doesn't work super well. That's mainly due to the fact that we don't have years to train the model. This is just to give you an idea of what's possible!\n",
    "\n",
    "### York Task (optional)\n",
    "Currently, this GAN uses Dense layers (hmmm, but it's using images). Try to convert the network to uses convolution, and see if you get better results! You'll probably need the following Keras layer types:\n",
    "1. Conv2d\n",
    "2. UpSampling2D or Conv2dTranspose\n",
    "3. Leaky ReLU (tends to work better for this task)\n",
    "4. (Maybe) BatchNormalization\n",
    "\n",
    "In general, you want the generator network to increase in size and the discriminator to decrease in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5923, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Load data!\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Pick only one digit. Let's go with 0 for now. You can try any digit you want!\n",
    "# This is done strictly for training speed -- the network works on all digits!\n",
    "x_train = x_train[np.where(y_train == 0)]\n",
    "print(x_train.shape)\n",
    "x_train = x_train.astype(np.float32)\n",
    "# Center data at 0 this time. Why???\n",
    "# x_train = (x_train - 127.5)/127.5\n",
    "x_train /= 255.0\n",
    "im_width, im_height = x_train[0].shape\n",
    "x_train_flat = x_train.reshape(len(x_train), im_width*im_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_47 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 256)               25856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_91 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_92 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_93 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 784)               803600    \n",
      "=================================================================\n",
      "Total params: 1,486,352\n",
      "Trainable params: 1,486,352\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Generator Network:\n",
    "g_in = layers.Input(shape=(100,))\n",
    "\n",
    "g_out = layers.Dense(256)(g_in)\n",
    "g_out = layers.LeakyReLU(0.2)(g_out)\n",
    "\n",
    "g_out = layers.Dense(512)(g_out)\n",
    "g_out = layers.LeakyReLU(0.2)(g_out)\n",
    "\n",
    "g_out = layers.Dense(1024)(g_out)\n",
    "g_out = layers.LeakyReLU(0.2)(g_out)\n",
    "\n",
    "# Maybe add more dense layers?\n",
    "g_out = layers.Dense(784, activation='sigmoid')(g_out)\n",
    "\n",
    "generator = models.Model(inputs=g_in, outputs=g_out)\n",
    "generator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam())\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_48 (InputLayer)        [(None, 784)]             0         \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 1024)              803840    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_94 (LeakyReLU)   (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_127 (Dense)            (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_95 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_96 (LeakyReLU)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 1,460,225\n",
      "Trainable params: 1,460,225\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the Discriminator Network\n",
    "d_in = layers.Input(shape=(784,))\n",
    "\n",
    "d_out = layers.Dense(1024)(d_in)\n",
    "d_out = layers.LeakyReLU(0.2)(d_out)\n",
    "d_out = layers.Dropout(0.3)(d_out)\n",
    "\n",
    "d_out = layers.Dense(512)(d_out)\n",
    "d_out = layers.LeakyReLU(0.2)(d_out)\n",
    "d_out = layers.Dropout(0.3)(d_out)\n",
    "\n",
    "d_out = layers.Dense(256)(d_out)\n",
    "d_out = layers.LeakyReLU(0.2)(d_out)\n",
    "\n",
    "d_out = layers.Dense(1, activation='sigmoid')(d_out)\n",
    "\n",
    "discriminator = models.Model(inputs=d_in, outputs=d_out)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=optimizers.Adam())\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_49 (InputLayer)        [(None, 100)]             0         \n",
      "_________________________________________________________________\n",
      "model_46 (Model)             (None, 784)               1486352   \n",
      "_________________________________________________________________\n",
      "model_47 (Model)             (None, 1)                 1460225   \n",
      "=================================================================\n",
      "Total params: 2,946,577\n",
      "Trainable params: 1,486,352\n",
      "Non-trainable params: 1,460,225\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the GAN Model\n",
    "discriminator.trainable = False\n",
    "gan_in = layers.Input(shape=(100,))\n",
    "fake_img = generator(gan_in)\n",
    "gan_out = discriminator(fake_img)\n",
    "gan= models.Model(inputs=gan_in, outputs=gan_out)\n",
    "gan.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "gan.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util Plot Function\n",
    "def plot_generated_images(epoch, generator, examples=100, dim=(10,10), figsize=(10,10)):\n",
    "    noise = np.random.normal(loc=0, scale=1, size=[examples, 100])\n",
    "    generated_images = generator.predict(noise)\n",
    "    generated_images = generated_images.reshape(100,28,28)\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0], dim[1], i+1)\n",
    "        plt.imshow(generated_images[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1\n",
      "Starting epoch 2\n",
      "Starting epoch 3\n",
      "Starting epoch 4\n",
      "Starting epoch 5\n",
      "Starting epoch 6\n",
      "Starting epoch 7\n",
      "Starting epoch 8\n",
      "Starting epoch 9\n",
      "Starting epoch 10\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### MODEL PARAMETERS ###\n",
    "epochs = 10\n",
    "batch_size = 96\n",
    "\n",
    "# The main Training Loop.\n",
    "# The GAN requires a manual loop of gradient descent as we need to generate the model inputs.\n",
    "for e in range(1, epochs + 1):\n",
    "    print(\"Starting epoch\", e)\n",
    "    for _ in range(int(len(x_train) / batch_size)):\n",
    "        # generate  random noise as an input  to  initialize the  generator\n",
    "        noise = np.random.normal(0,1, [batch_size, 100])\n",
    "        # Generate fake MNIST digits from noised input\n",
    "        generated_images = generator.predict(noise)\n",
    "        # Get a random set of  real images\n",
    "        img_batch = x_train_flat[np.random.randint(low=0,high=x_train_flat.shape[0],size=batch_size)]\n",
    "        # Concatenate into one dataset \n",
    "        X = np.concatenate([img_batch, generated_images])\n",
    "        # Create the labels (note we dont use probability 1, that would give the discriminator too much power!)\n",
    "        y =np.zeros(2*batch_size)\n",
    "        y[:batch_size]= 0.9\n",
    "        # train discriminator onfake and real data to make it better at telling them apart.\n",
    "        discriminator.trainable = True\n",
    "        discriminator.train_on_batch(X, y)\n",
    "\n",
    "        # Generate more noise for the gan\n",
    "        noise = np.random.normal(0,1, [batch_size, 100])\n",
    "        y_gen = np.ones(batch_size)\n",
    "\n",
    "        # Fix the weights of the discriminator.\n",
    "        discriminator.trainable=False\n",
    "\n",
    "        # Train the Generative network!\n",
    "        # we do this by training the whole GAN with the discriminator weights frozen\n",
    "        # This is becasue we need the entire network.\n",
    "        gan.train_on_batch(noise, y_gen)\n",
    "    \n",
    "plot_generated_images(e, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congrats! You made it to the end!\n",
    "Feel free to ask me any other questions you might have about machine learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
